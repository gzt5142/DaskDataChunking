{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rechunking Larger Datasets with Dask\n",
    "\n",
    "The goal of this notebook is to expand on the rechunking performed in the [Introductory Rechunking tutorial](../101/Rechunking.ipynb).\n",
    "This notebook will perfrom the same operations, but will work on the **much** larger dataset and involve some parallelization using [Dask](https://www.dask.org/). \n",
    "\n",
    ":::{Warning}\n",
    "You should only run workflows like this tutorial on a cloud or HPC compute node.\n",
    "In application, this will require reading and writing **enormous** amounts of data.\n",
    "Using a typical network connection and simple compute environment, you would saturate your bandwidth and max out your processor, thereby taking days to for the rechunking to complete.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "from rechunker import rechunk\n",
    "import zarr\n",
    "import shutil\n",
    "import numpy as np\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in a Zarr Store\n",
    "\n",
    "Like the [Introductory Rechunking tutorial](../101/Rechunking.ipynb), we will use the data from the National Water Model Retrospective Version 2.1.\n",
    "The full dataset is part of the [AWS Open Data Program](https://aws.amazon.com/opendata/), available via the S3 bucket at: `s3://noaa-nwm-retro-v2-zarr-pds/`.\n",
    "\n",
    "As this is a `zarr` store, let's read it in with [`xarray.open_dataset()`](https://docs.xarray.dev/en/stable/generated/xarray.open_dataset.html) and `engine='zarr'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = fsspec.get_mapper('s3://noaa-nwm-retro-v2-zarr-pds', anon=True)\n",
    "ds = xr.open_dataset(file, chunks={}, engine='zarr')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict for Tutorial\n",
    "\n",
    "As we saw in the [Introductory Rechunking tutorial](../101/Rechunking.ipynb), this data set is massive, taking up almost 32 TiB uncompressed.\n",
    "As this is a tutorial, we will still restrict the data to a subset, as we don't really need to work on the entire dataset.\n",
    "Following the [Introductory Rechunking tutorial](../101/Rechunking.ipynb) let's only look at `streamflow` and `velocity` for the first 15,000 `feature_id`, but the 2000s decade of water years (October 1999 through September 2009) instead of a single water year.\n",
    "This will make our dataset larger-than-memory, but it should still run in a reasonable amount of time.\n",
    "\n",
    "For processing the full-sized dataset, you'd just skip this step where we slice off a representative example of the data.\n",
    "Expect run time to increase in proportion to the size of the data being processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[['streamflow', 'velocity']]\n",
    "ds = ds.isel(feature_id=slice(0, 15000))\n",
    "ds = ds.sel(time=slice('1999-10-01', '2009-09-30'))\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our subset of data is only about 10 GiB per data variable and has a chunk shape of `{'time': 672, 'feature_id': 15000}` with size of 76.9 MiB.\n",
    "However, the chunk shape is not an optimal choice for our analysis as it is chunked completely by `feature_id` (i.e., all feature IDs for a given time can be read in a single chunk).\n",
    "Following the [Introductory Rechunking tutorial](../101/Rechunking.ipynb), let's get chunk shapes that are time-series wise chunking (i.e., all `time` for a given `feature_id` in one chunk) for streamflow and balanced for velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rechunk Plan\n",
    "\n",
    "Using our general strategy of time-series wise chunking for streamflow and balanced for velocity,\n",
    "let's compute how large the chunk sizes will be if we have chunk shapes of `{'time': 87672, 'feature_id': 1}` for streamflow and 3 chunks per dimension for velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeature = len(ds.feature_id)\n",
    "ntime = len(ds.time)\n",
    "\n",
    "streamflow_chunk_plan = {'time': ntime, 'feature_id': 1}\n",
    "bytes_per_value = ds.streamflow.dtype.itemsize\n",
    "total_bytes = streamflow_chunk_plan['time'] * streamflow_chunk_plan['feature_id'] * bytes_per_value\n",
    "streamflow_MiB = total_bytes / (2 ** 10) ** 2\n",
    "partial_chunks = {'time': ntime -  streamflow_chunk_plan['time'] * (ntime / streamflow_chunk_plan['time']),\n",
    "                  'feature_id': nfeature -  streamflow_chunk_plan['feature_id'] * (nfeature / streamflow_chunk_plan['feature_id']),}\n",
    "print(\"STREAMFLOW \\n\"\n",
    "      f\"Chunk of shape {streamflow_chunk_plan} \\n\"\n",
    "      f\"Partial 'time' chunk remainder: {partial_chunks['time']} ({partial_chunks['time']/streamflow_chunk_plan['time']:.3f}% of a chunk)\\n\"\n",
    "      f\"Partial 'feature_id' chunk remainder: {partial_chunks['feature_id']} ({partial_chunks['feature_id']/streamflow_chunk_plan['feature_id']:.3f}% of a chunk)\\n\"\n",
    "      f\"Chunk size: {streamflow_MiB:.2f} [MiB] \\n\")\n",
    "\n",
    "chunks_per_dim = 3\n",
    "velocity_chunk_plan = {'time': ntime // chunks_per_dim, 'feature_id': nfeature // chunks_per_dim}\n",
    "bytes_per_value = ds.velocity.dtype.itemsize\n",
    "total_bytes = velocity_chunk_plan['time'] * velocity_chunk_plan['feature_id'] * bytes_per_value\n",
    "velocity_MiB = total_bytes / (2 ** 10) ** 2\n",
    "partial_chunks = {'time': ntime -  velocity_chunk_plan['time'] * chunks_per_dim,\n",
    "                  'feature_id': nfeature -  velocity_chunk_plan['feature_id'] * chunks_per_dim,}\n",
    "print(\"VELOCITY \\n\"\n",
    "      f\"Chunk of shape {velocity_chunk_plan} \\n\"\n",
    "      f\"Partial 'time' chunk remainder: {partial_chunks['time']} ({partial_chunks['time']/velocity_chunk_plan['time']:.3f}% of a chunk)\\n\"\n",
    "      f\"Partial 'feature_id' chunk remainder: {partial_chunks['feature_id']} ({partial_chunks['feature_id']/velocity_chunk_plan['feature_id']:.3f}% of a chunk)\\n\"\n",
    "      f\"Chunk size: {velocity_MiB:.2f} [MiB]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we can see that the streamflow chunk size is way to small by a factor of ~100.\n",
    "So, let's include 100 feature IDs per chunk.\n",
    "As for velocity, it is about ~10x too big.\n",
    "As it is an even chunk split, that means we need to increase the number of chunks per dimension by ~$\\sqrt{10} \\approx 3$.\n",
    "However knowing that the time dimension is hourly, we can get no partial chunks if our chunk per dimension is a divisor of 24.\n",
    "Luckily, this also applies to the feature ID dimension as 15000 is a multiple of 24.\n",
    "So, rather than increasing our chunks per dimension by a factor of 3 to 9, let's increase them to 12 as this will give no partial chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeature = len(ds.feature_id)\n",
    "ntime = len(ds.time)\n",
    "\n",
    "streamflow_chunk_plan = {'time': ntime, 'feature_id': 100}\n",
    "bytes_per_value = ds.streamflow.dtype.itemsize\n",
    "total_bytes = streamflow_chunk_plan['time'] * streamflow_chunk_plan['feature_id'] * bytes_per_value\n",
    "streamflow_MiB = total_bytes / (2 ** 10) ** 2\n",
    "partial_chunks = {'time': ntime -  streamflow_chunk_plan['time'] * (ntime / streamflow_chunk_plan['time']),\n",
    "                  'feature_id': nfeature -  streamflow_chunk_plan['feature_id'] * (nfeature / streamflow_chunk_plan['feature_id']),}\n",
    "print(\"STREAMFLOW \\n\"\n",
    "      f\"Chunk of shape {streamflow_chunk_plan} \\n\"\n",
    "      f\"Partial 'time' chunk remainder: {partial_chunks['time']} ({partial_chunks['time']/streamflow_chunk_plan['time']:.3f}% of a chunk)\\n\"\n",
    "      f\"Partial 'feature_id' chunk remainder: {partial_chunks['feature_id']} ({partial_chunks['feature_id']/streamflow_chunk_plan['feature_id']:.3f}% of a chunk)\\n\"\n",
    "      f\"Chunk size: {streamflow_MiB:.2f} [MiB] \\n\")\n",
    "\n",
    "chunks_per_dim = 12\n",
    "velocity_chunk_plan = {'time': ntime // chunks_per_dim, 'feature_id': nfeature // chunks_per_dim}\n",
    "bytes_per_value = ds.velocity.dtype.itemsize\n",
    "total_bytes = velocity_chunk_plan['time'] * velocity_chunk_plan['feature_id'] * bytes_per_value\n",
    "velocity_MiB = total_bytes / (2 ** 10) ** 2\n",
    "partial_chunks = {'time': ntime -  velocity_chunk_plan['time'] * chunks_per_dim,\n",
    "                  'feature_id': nfeature -  velocity_chunk_plan['feature_id'] * chunks_per_dim,}\n",
    "print(\"VELOCITY \\n\"\n",
    "      f\"Chunk of shape {velocity_chunk_plan} \\n\"\n",
    "      f\"Partial 'time' chunk remainder: {partial_chunks['time']} ({partial_chunks['time']/velocity_chunk_plan['time']:.3f}% of a chunk)\\n\"\n",
    "      f\"Partial 'feature_id' chunk remainder: {partial_chunks['feature_id']} ({partial_chunks['feature_id']/velocity_chunk_plan['feature_id']:.3f}% of a chunk)\\n\"\n",
    "      f\"Chunk size: {velocity_MiB:.2f} [MiB]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!\n",
    "Now, our chunks are a reasonable size and have no remainders.\n",
    "So, lets use these chunk plans for our rechunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_plan = {\n",
    "    'streamflow': streamflow_chunk_plan,\n",
    "    'velocity': velocity_chunk_plan,\n",
    "     # We don't want any of the coordinates chunked\n",
    "    'latitude': (nfeature,),\n",
    "    'longitude': (nfeature,),    \n",
    "    'time': (ntime,),\n",
    "    'feature_id': (nfeature,)\n",
    "}\n",
    "chunk_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rechunk with `Rechunker`\n",
    "\n",
    "With this plan, we can now ask `rechunker` to re-write the data using the prescribed chunking pattern.\n",
    "\n",
    "### Set up output location\n",
    "\n",
    "Unlike with the smaller dataset in our previous rechunking tutorial, we will write this larger dataset to an object store (an S3 'bucket') in a datacenter.\n",
    "So, we need to set that up so that `rechunker` will have a suitable place to write data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Update these next three cells to properly use AWS or HPC. May need to add some markdown cells to describe what is being done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_PROFILE'] = \"osn-renci\"\n",
    "os.environ['AWS_S3_ENDPOINT'] = \"https://renc.osn.xsede.org\"\n",
    "# %run ../AWS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from getpass import getuser\n",
    "uname=getuser()\n",
    "\n",
    "fsw = fsspec.filesystem(\n",
    "    's3', \n",
    "    anon=False, \n",
    "    default_fill_cache=False, \n",
    "    skip_instance_cache=True, \n",
    "    client_kwargs={'endpoint_url': os.environ['AWS_S3_ENDPOINT'], }\n",
    ")\n",
    "\n",
    "workspace = 's3://rsignellbucket2/'\n",
    "testDir = workspace + \"testing/\"\n",
    "myDir = testDir + f'{uname}/'\n",
    "fsw.mkdir(testDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_store = fsw.get_mapper(myDir + 'tutorial_staging.zarr')\n",
    "outfile = fsw.get_mapper(myDir + 'tutorial_rechunked.zarr')\n",
    "for fname in [staging, outfile]:\n",
    "    print(f\"Ensuring {fname.root} is empty...\", end='')\n",
    "    try:\n",
    "        fsw.rm(fname.root, recursive=True)\n",
    "    except:\n",
    "        FileNotFoundError\n",
    "    print(\" Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spin up Dask Cluster\n",
    "\n",
    "Our rechunking operation will be able to work in parallel.\n",
    "To do that, we will spin up a `dask` cluster on the cloud hardware to schedule the various workers.\n",
    "Note that this cluster must be configured with a specific user **profile** with permissions to write to our eventual output location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Ensure this is spinning up the cluster we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "# client = Client(n_workers=8, silence_logs=logging.ERROR)\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rechunk\n",
    "\n",
    "Now, we are ready to rechunk!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = rechunk(\n",
    "    # Make sure the base chunks are correct\n",
    "    ds.chunk({'time': 672, 'feature_id': 15000}),\n",
    "    target_chunks=chunk_plan,\n",
    "    max_mem=\"16GB\",\n",
    "    target_store=outfile,\n",
    "    temp_store=temp_store\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that merely invoking Rechunker does not do any work.\n",
    "It just sorts out the rechunking plan and writes metadata.\n",
    "We need to call `.execute` on the `results` object to actually run the rechunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import progress, performance_report\n",
    "\n",
    "with performance_report(filename=\"dask-report.html\"):\n",
    "    r = result.execute(retries=10)  \n",
    "\n",
    "# Also consolidate the metadata for fast reading into xarray\n",
    "_ = zarr.consolidate_metadata(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Let's read in the resulting re-chunked dataset to see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rechunked = xr.open_zarr(outfile)\n",
    "ds_rechunked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before:\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After:\n",
    "ds_rechunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "4100cc85ffefb381c538d28dd18cb927e5a99f05bbed6aaad5313d7bb1c2079e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
