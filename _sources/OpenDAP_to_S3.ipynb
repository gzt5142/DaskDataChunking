{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing OpenDAP Data to ZARR on S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The objective demonstrated here is to take a dataset which is already published via an OpenDAP endpoint and write it to S3 in zarr format. The end goal is to make the given dataset 'cloud optimized' for workflows operating in cloud compute environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "This is all stuff we are going to need: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import configparser\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n",
    "\n",
    "import xarray as xr\n",
    "import dask\n",
    "import fsspec\n",
    "import rechunker\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Python     :\", sys.version) \n",
    "from importlib.metadata import version as _ver\n",
    "for m in ['numpy', 'xarray', 'dask', 'zarr', 'fsspec', 's3fs', 'rechunker']:\n",
    "    print(f\"{m:10s} : {_ver(m)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Credentials\n",
    "Because we will be writing to an S3 object store, we need credentials.  This notebook will assume that the correct credentials are already stored in `~/.aws/credentials` . \n",
    "\n",
    "I am using a profile to write to the OSN storage device (profile name `osn-renci`). If you run this notebook and want to write elsewhere with other credentials, you may change the profile name and endpoint in the cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awsconfig = configparser.ConfigParser()\n",
    "awsconfig.read(\n",
    "    os.path.expanduser('~/.aws/credentials') \n",
    "    # default location... if yours is elsewhere, change this.\n",
    ")\n",
    "_profile_nm  = os.environ.get('AWS_PROFILE', 'osn-renci')\n",
    "_endpoint = os.environ.get('AWS_S3_ENDPOINT', 'https://renc.osn.xsede.org')\n",
    "# Set environment vars based on parsed awsconfig\n",
    "try:\n",
    "    os.environ['AWS_ACCESS_KEY_ID']     = awsconfig[_profile_nm]['aws_access_key_id']\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = awsconfig[_profile_nm]['aws_secret_access_key']\n",
    "    os.environ['AWS_S3_ENDPOINT']       = _endpoint\n",
    "    os.environ['AWS_PROFILE'] = _profile_nm\n",
    "    os.environ['AWS_DEFAULT_PROFILE'] = _profile_nm\n",
    "except KeyError:\n",
    "    logging.error(\"Problem parsing the AWS credentials file. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Souce and Output Location\n",
    "\n",
    "This demonstration will use the PRISM (v2) dataset available from <https://cida.usgs.gov> as the example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: \n",
    "OPENDAP_url = 'https://cida.usgs.gov/thredds/dodsC/prism_v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which we will write to the OSN pod: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT Location: \n",
    "workspace = f's3://rsignellbucket2/testing/prism/'\n",
    "\n",
    "# OUTPUT Dataset Name:\n",
    "FNAME = 'PRISM_v2.zarr'\n",
    "\n",
    "# Instantiate a fsspec reference to the workspace: \n",
    "fsw = fsspec.filesystem('s3', \n",
    "    anon=False, \n",
    "    default_fill_cache=False, \n",
    "    skip_instance_cache=True, \n",
    "    client_kwargs={ 'endpoint_url': os.environ['AWS_S3_ENDPOINT'] },\n",
    ") # this will take credentials from the environment variables, \n",
    "# as defined above. No need to specify profile or keys. The endpoint, \n",
    "# unfortunately is necessary to name explicitly.\n",
    "\n",
    "outdir = workspace + FNAME\n",
    "target_store = fsw.get_mapper(outdir)\n",
    "\n",
    "try:\n",
    "    if fsw.exists(workspace + FNAME):\n",
    "        logging.warning(\"Removing existing file/folder: %s\", fname)\n",
    "        fsw.rm(workspace + fname, recursive=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"READY !!\")\n",
    "\n",
    "fsw.ls(workspace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Data\n",
    "\n",
    "Now that we have everything set up, let's finally have a look at the data that we're working on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_in = xr.open_dataset(OPENDAP_url, decode_times=False)\n",
    "ds_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset holds four variables (setting aside `time_bnds` for the moment -- special case). The variables are `tmx`, `ppt`, and `tmn`, which are associated with three indices by which data values are located in space and time (`lat`, `lon`, and `time`).\n",
    "\n",
    "Looking at the \"Dimensions\" line, you can see that each of these dimensions is quantified -- how many unique values are available in each dimension: \n",
    "\n",
    "* **lon** = 1405\n",
    "* **lat** = 621\n",
    "* **time** = 1512\n",
    "\n",
    "We will need these numbers later, so take note. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of the data variables to learn more about how it is presented by the OPeNDAP endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_in.tmn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note from the top line that this variable is indexed as a tuple in `(time, lat, lon)`.\n",
    "\n",
    "Also note that in the data attributes, that `_ChunkSizes` gives the chunk sizes of the data, expressed as a tuple to match the dimensions.  This data is broken into chunks, each of which is \n",
    "* 1 timestep, \n",
    "* 23 latitude steps, and \n",
    "* 44 longitude steps. \n",
    "\n",
    "Gven that `tmn` is stored as a `float32` (4 bytes), each chunk is of size: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes = 1 * 23 * 44 * 4\n",
    "kbytes = bytes / (2**10)\n",
    "mbytes = kbytes / (2**10)\n",
    "print(f\"TMN chunk size: {bytes=} ({kbytes=:.2f})({mbytes=:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an **extremely** small chunk size, and not at all suitable for cloud storage.  \n",
    "\n",
    "The good news is that we are not stuck with it. The opendap server is offering us its default chunking for network API requests, but this is configurable. We can change it to something more suitable.  We will definitely do this later. Take note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Preferred Chunking Plan\n",
    "\n",
    "Given what we know about this data, we can apply some cloud storage principles to form a strategy for how best to chunk the data when we write it to S3. Broadly, we need to specify chunk **size** and chunk **shape**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size\n",
    "it's a balance.. here are some constraints: \n",
    "* Files Too Big -- In a zarr dataset, each chunk is stored as a separate file. If we \n",
    "  need data from a chunk, no matter how little or how much, that file gets \n",
    "  opened, decompressed, and the whole thing read in. Large chunk sizes means that there may be \n",
    "  a lot of data transferred in situations when only a subset of that chunk's \n",
    "  data is needed. (\"sharding\" is coming in zarr v3, but for now, this is how it works). \n",
    "* Files Too Small -- If the chunk size is too small, the time it takes to read and \n",
    "  decompress the data for each chunk can become comparable to the latency of S3 (typically 10-100ms). We want the reads to take at least a second or so, so that the latency is not a significant part of the overall timing. \n",
    "  \n",
    "As a general rule, aim for chunk sizes between 10 and 200MB, depending on shape\n",
    "and expected read pattern (see below). Expect 100ms latency for each separate\n",
    "chunk that a process needs to open.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape\n",
    "\n",
    "The preferred shape of each chunk will depend on the read pattern for future analyses. For some datasets, this will be very apparent (NWIS gages, for example -- it very likely will be consumed along the time dimension most of the time).  For datasets where there is no clear preference, we can try to chunk based on likely read patterns, but allow for other patterns without too much of a performance penalty. \n",
    "Let's see how we might do this for our sample dataset.  This data will likely be used in one of two dominant read patterns: \n",
    "\n",
    "* Time series for a given location (or small spatial extent)\n",
    "  * As a special case -- is it likely that time series will be subset (by year? month?)\n",
    "* Full extent for a given point in time. \n",
    "  * As a special case -- are specific study areas more used than others\n",
    "\n",
    "\n",
    "#### TIME dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1512 time steps.... what happens if we chunk in twelves (i.e. a year at a time)\n",
    "print(\"We need {} chunks.\".format(1512 // 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, a user could get an entire year of this monthly data as a single chunk.  If they wanted a full time series across the entire dataset, they would need to read 126 chunks. \n",
    "\n",
    "So this is where the judgement call gets made -- which is the more likely read pattern for time?  Year-by-year, or the whole time set (or some sequence of a few years). In this case, I think it is more likely to want more than just one year's data.  A happy medium for chunk size is 6 years of data per chunk: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chunk_size = 12*6\n",
    "print(\"TIME chunking: {} chunks of size {}\".format(1512 / test_chunk_size, test_chunk_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pattern means only 21 chunks instead of the original 126 for a full time series in a given location. The max latency (using 200ms per read as the theoretical expectation) to read all of that data on a single worker is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Expected latency in seconds: \", (21 * 200) * 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for cluster-aware analyses, multiple chunks can be read at the same time. Total wall-clock time will be reduced in that case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPACE\n",
    "\n",
    "We're realy chunking in dimensions -- and there are two dimensions to this dataset which contribute to \"space\": `lat` and `lon`.  These can have separate chunk sizes. The question to ask is whether future users of this data will want square \"tiles\" in space, or will they want equal numbers of longitude and latitude?  (That is, is it important that the North-South extent be broken into the same number of chunks as the East-West extent?).  I'll be breaking this into square tiles, so there will be more `lon` chunks than `lat` chunks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the dataset: \n",
    "lon=1405\n",
    "lat=621\n",
    "test_chunk_size = lat // 4 # split the smaller of the two dimensions into 4 chunks\n",
    "print(\"LON chunking: {} chunks of size {}\".format(lon / test_chunk_size, test_chunk_size))\n",
    "print(\"LAT chunking: {} chunks of size {}\".format(lat / test_chunk_size, test_chunk_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that we have **just over** a round number of chunks.  This means that the last chunk in the given dimension will be extremely thin. In the case of that latitude chunk size, the extra 0.006 of a chunk means that the last chunk is only one `lat` observation. This all but guarantees that two chunks are needed for a small spatial extent near the \"end\" of the dimension. Ideally, we would want partial chunks to be at least half the size of the standard chunk.  The bigger that 'remainder' fraction, the better. \n",
    "\n",
    "Let's adjust numbers a little: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chunk_size = 160\n",
    "print(\"LON chunking: {} chunks of size {}\".format(lon / test_chunk_size, test_chunk_size))\n",
    "print(\"LAT chunking: {} chunks of size {}\".format(lat / test_chunk_size, test_chunk_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this pattern, the \"remainder\" latitude chunk will be 141 (125 for the last longitude chunk).  All others will be a square 160. \n",
    "\n",
    "The entire spatial extent for a single time step can be read in 36 chunks, with this pattern. That feels a little high to me, given that this dataset will likely be taken at full extent.  Let's go a little bigger: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chunk_size = 354\n",
    "print(\"LON chunking: {} chunks of size {}\".format(lon / test_chunk_size, test_chunk_size))\n",
    "print(\"LAT chunking: {} chunks of size {}\".format(lat / test_chunk_size, test_chunk_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not *quite* as good, in terms of full-chunk remainders, but the whole extent can be had in only 8 chunks. \n",
    "\n",
    "Note, that if were really confident that most analyses wanted the full extent, we would just put the whole lat/lon dimensions into single chunks each. This would ensure (and **require**) that we read the entire extent any time we wanted any **part** of the extent for a given timestep.  Our poor time-series analysis would then be stuck reading the entire dataset to get all time values for a single location. `:sadface:`\n",
    "\n",
    "### Total Chunk Size\n",
    "Now that we have a rough idea of the chunk dimensions, let's compute its size in bytes.  This will tell us if we've hit our target of between 10 and 200Mb per chunk.  More importantly, it will tell us if we will overwhelm the OpenDAP server -- the server can only give us 500MB at a time. Chunks should really be smaller than this (which we want anyway, but this 500MB limit is a hard cut-off). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#       lat   lon  time  float32\n",
    "bytes = 354 * 354 * 72 * 4\n",
    "kbytes = bytes / (2**10)\n",
    "mbytes = kbytes / (2**10)\n",
    "print(f\"TMN chunk size: {bytes=} ## {kbytes=:.2f} ## {mbytes=:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're looking really good for size.  Maybe even a bit low.  But we're in the (admitedly broad) range of 10-200. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the chunk plan\n",
    "Now that we know how we want to chunk the data, we need to give that information to the API which will read the data from its OpenDAP endpoint: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_in = xr.open_dataset(OPENDAP_url, decode_times=False, chunks={'time': 72, 'lon': 354, 'lat': 354})\n",
    "ds_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking more closely at the `tmn` variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_in.tmn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** that what it is giving us for `_ChunkSizes` in the attributes is irrelevant now.  We've specifically asked the dask interface to request this data according to the chunk pattern specified -- and revealed in the graphical display.  Dask will make specific OPeNDAP requests *per chunk* using appropriate query parameters to the server. \n",
    "\n",
    "Because this chunk pattern can be provided by the server, and it is a reasonable pattern for object storage in S3, we do **not** need to add the complexity of `rechunker`. We can just have the zarr driver write it out according to the same plan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_plan = {\n",
    "    'ppt':{'time': 72, 'lon': 354, 'lat': 354},    \n",
    "    'tmx':{'time': 72, 'lon': 354, 'lat': 354},    \n",
    "    'tnm':{'time': 72, 'lon': 354, 'lat': 354},\n",
    "    'time_bnds': {'time': 1, 'tbnd': 2},\n",
    "    'lat': (621,),\n",
    "    'lon': (1405,),\n",
    "    'time': (1512,),\n",
    "    'tbnd': (2,)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the coordinate variables (`lat`, `lon`, etc) are stored as single-chunk stripes of data. The index will always be needed in its entirity, so we chunk it such that it reads with one chunk. \n",
    "\n",
    "Also note -- I gave the full chunk plan **AS IF** we were going to instruct at tool like `rechunker` to chunk in a particular way.  This pattern is already the shape our data is in, because that's how we asked for it from OPeNDAP.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to Write\n",
    "\n",
    "OK... finally we are ready to write out our data. Note that the input data is not in memory, but we pretend that it is.  This is one of the many advantages of using dask arrays.  It will fetch the necessary chunks when the data in them is needed. And the good news about that is that Dask is capable of doing these operations *in parallel* and *without hand-holding*.  We don't have to design a parallel workflow: Dask will sort it out.  BUT... to take advantage of that parallelism, we need to start up a cluster: \n",
    "\n",
    "### Start Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import progress, performance_report\n",
    "try:\n",
    "    from dask_gateway import Gateway\n",
    "except ImportError:\n",
    "    logging.error(\"Unable to import Dask Gateway.  Are you running in a cloud compute environment?\\n\")\n",
    "    raise\n",
    "os.environ['DASK_DISTRIBUTED__SCHEDULER__WORKER_SATURATION'] = \"1.0\"\n",
    "\n",
    "gateway = Gateway()\n",
    "_options = gateway.cluster_options()\n",
    "_options.conda_environment='users/users-pangeo'  ##<< this is the conda environment we use on nebari.\n",
    "_options.profile = 'Medium Worker'\n",
    "_env_to_add={}\n",
    "aws_env_vars=['AWS_ACCESS_KEY_ID',\n",
    "              'AWS_SECRET_ACCESS_KEY',\n",
    "              'AWS_SESSION_TOKEN',\n",
    "              'AWS_DEFAULT_REGION',\n",
    "              'AWS_S3_ENDPOINT']\n",
    "for _e in aws_env_vars:\n",
    "    if _e in os.environ:\n",
    "        _env_to_add[_e] = os.environ[_e]\n",
    "_options.environment_vars = _env_to_add    \n",
    "cluster = gateway.new_cluster(_options)          ##<< create cluster via the dask gateway\n",
    "cluster.adapt(minimum=10, maximum=30)             ##<< Sets scaling parameters. \n",
    "client = cluster.get_client()\n",
    "\n",
    "print(\"The 'cluster' object can be used to adjust cluster behavior.  i.e. 'cluster.adapt(minimum=10)'\")\n",
    "print(\"The 'client' object can be used to directly interact with the cluster.  i.e. 'client.submit(func)' \")\n",
    "print(f\"The link to view the client dashboard is:\\n>  {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_zarr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with performance_report('./reports/OpenDAP_to_S3-perfreport.html'):\n",
    "    ds_in.to_zarr(target_store, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify\n",
    "To make sure that we really wrote the whole thing to S3, let's sample the data for some simple plots: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = xr.open_dataset(target_store, engine='zarr', chunks={})\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds.ppt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "new_ds.ppt.sel(lon=-75, lat=41.1, method='nearest').load().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds.tmx.sel(time=\"1970-01\").load().hvplot(x='lon', y='lat', rasterize=True, geo=True, tiles='OSM' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close down cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
