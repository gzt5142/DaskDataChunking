{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size and Shape of Chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**OBJECTIVE**:  \n",
    "The objective of this chapter is to demonstrate how to read an existing dataset available as an OpenDAP endpoint, and translate it into a cloud-optimized zarr on S3. \n",
    "\n",
    "This notebook will touch on chunking considerations -- Factors to think about when deciding how to break up the data into chunks/bites. And why you would want to do that to begin with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import xarray as xr\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%run ../utils.ipynb\n",
    "_versions(['xarray'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Chunk\n",
    "The simple reason is that the full dataset won't fit in memory. It has to be\n",
    "divided in some way so that only those parts of the data being actively worked\n",
    "are loaded. \n",
    "\n",
    "This has other benefits when it comes to parallel algorithms.  If work can be\n",
    "performed in parallel, it is easy to set it up such that a separate worker is\n",
    "assigned to each chunk of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Data\n",
    "We're going to keep looking at the sample PRISM dataset, as read from \n",
    "an OpenDAP endpoint: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: \n",
    "OPENDAP_url = 'https://cida.usgs.gov/thredds/dodsC/prism_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_in = xr.open_dataset(OPENDAP_url, decode_times=False)\n",
    "ds_in.tmn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given what we know about this data, we can apply some cloud storage principles to form a strategy for how best to chunk the data when we write it to S3. Broadly, we need to specify chunk **size** and chunk **shape**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape Considerations\n",
    "\n",
    "Shape refers to how to divide an array along each dimension.  So we will need to decide on the chunk size for each of the dimensions of this data.  \n",
    "\n",
    "The preferred shape of each chunk will depend on the read pattern for future analyses.\n",
    "We will be chunking the data so that future reads will be performant -- and that depends on whether\n",
    "the data favors one dimension or another.  For some datasets, this will be very apparent (NWIS gages, \n",
    "for example -- it very likely will be consumed along the `time` dimension most often. It is _more likely_\n",
    "that future analysis of this data will take a time series for a given gage, as opposed to taking all\n",
    "gage data for a given time).  For datasets where there is no clear preference, we can try to chunk \n",
    "based on likely read patterns, but allow for other patterns without too much of a performance penalty. \n",
    "\n",
    "Let's see how we might do this for our sample dataset.  This data will likely be used in one of two \n",
    "dominant read patterns: \n",
    "\n",
    "* Time series for a given location (or small spatial extent)\n",
    "  * As a special case -- is it likely that time series will be subset by a logical unit? e.g. will this\n",
    "    monthly data be consumed in blocks of 12 (yearly)? \n",
    "* Full extent for a given point in time. \n",
    "  * As a special case -- are specific study areas more used than others? \n",
    "  \n",
    "Let's look at a couple of options for space and time chunking: \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Dimension\n",
    "\n",
    "The example dataset has 1512 time steps.  What happens if we chunk in groups of \n",
    "twelve (i.e. a year at a time)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We need {} chunks.\".format(1512 // 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, a user could get an single year of this monthly data as a single chunk.\n",
    "If they wanted a full time series across the entire dataset, they would need to read \n",
    "126 chunks. \n",
    "\n",
    "So this is where the judgement call gets made -- which is the more likely read pattern \n",
    "for time?  Year-by-year, or the whole time set (or some sequence of a few years). In \n",
    "this case, I think it is more likely to want more than just one year's data.  A happy \n",
    "medium for chunk size is 6 years of data per chunk: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chunk_size = 12*6\n",
    "print(\"TIME chunking: {} chunks of size {}\".format(1512 / test_chunk_size, test_chunk_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pattern means only 21 chunks (instead of the 126 chunks we were considering a moment ago) \n",
    "for a full time series in a given location. If we assume the rule-of-thumb latency for reading\n",
    "and processing a chunk (100ms per read as the theoretical expectation -- see \"size\" below), \n",
    "those 21 chunks can be read by a single worker in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Expected latency in seconds: \", (21 * 100) * 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for cluster-aware analyses, multiple chunks can be read at the same time. Total wall-clock time will be reduced in that case. With 21 chunks, maximum (theoretical) parallelism would be achieved with 21 cooperating workers.\n",
    "\n",
    "We'll move forward with the time dimension chunked into groups of **72** for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPACE\n",
    "\n",
    "We're realy chunking in dimensions -- and there are two dimensions to this dataset which \n",
    "contribute to \"space\": `lat` and `lon`.  These can have separate chunk sizes. The question \n",
    "to ask is whether future users of this data will want square \"tiles\" in space, or will \n",
    "they want proportionally-sized longitude and latitude?  That is, is it important that the \n",
    "North-South extent be broken into the same number of chunks as the East-West extent?).\n",
    "I'll be breaking this into square tiles, so there will be more `lon` chunks than `lat` chunks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the dataset: \n",
    "lon=1405\n",
    "lat=621\n",
    "test_chunk_size = lat // 4 # split the smaller of the two dimensions into 4 chunks\n",
    "print(\"LON chunking: {} chunks of size {}\".format(lon / test_chunk_size, test_chunk_size))\n",
    "print(\"LAT chunking: {} chunks of size {}\".format(lat / test_chunk_size, test_chunk_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that we have **just over** a round number of chunks.\n",
    "Having `9.06` longigutde chunks means we will have `10` chunks in practice, but \n",
    "that last one is not full-sized. In this case, this means that the last chunk \n",
    "in the given dimension will be extremely thin. \n",
    "\n",
    "In the case of that latitude chunk size, the extra `0.006` of a chunk means that\n",
    "the last, fractional, chunk is only one `lat` observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   remainder   chunksize\n",
    "0.0064516129 * 155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This all but guarantees that two chunks are needed for a small spatial extent \n",
    "near the \"end\" of the `lat` dimension. Ideally, we would want partial chunks \n",
    "to be at least half the size of the standard chunk.  The bigger that 'remainder' \n",
    "fraction, the better. \n",
    "\n",
    "Let's adjust numbers a little so that we don't have that sliver.  We're still\n",
    "committed to square tiles, so let's try a larger chunk size to change the size\n",
    "of that last fraction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chunk_size = 160\n",
    "print(\"LON chunking: {} chunks of size {}\".format(lon / test_chunk_size, test_chunk_size))\n",
    "print(\"LAT chunking: {} chunks of size {}\".format(lat / test_chunk_size, test_chunk_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this pattern, the \"remainder\" latitude chunk will be 141 in the `lat` dimension (125 \n",
    "for the last `lon` chunk).  All others will be a square 160 observations in both directions.\n",
    "\n",
    "This amounts to a 9x4 chunk grid, with the last chunk in each direction being partial. \n",
    "\n",
    "The entire spatial extent for a single time step can be read in 36 chunks, with this pattern. \n",
    "That feels a little high to me, given that this dataset will likely be taken at full extent\n",
    "for a typical analysis.  Let's go a little bigger to see what that gets us: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chunk_size = 354\n",
    "print(\"LON chunking: {} chunks of size {}\".format(lon / test_chunk_size, test_chunk_size))\n",
    "print(\"LAT chunking: {} chunks of size {}\".format(lat / test_chunk_size, test_chunk_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not *quite* as good in terms of full-chunk remainders -- but on the other hand, the whole extent \n",
    "can be had in only 8 chunks.  The smallest remainder is still 75% of a full-sized square tile, which is\n",
    "acceptable. \n",
    "\n",
    "Note, that if were really confident that most analyses wanted the full extent, we might\n",
    "be better off to just put the whole lat/lon dimensions into single chunks each. This \n",
    "would ensure (and **require**) that we read the entire extent any time we wanted any **part** \n",
    "of the extent for a given timestep.  Our poor time-series analysis would then be stuck \n",
    "reading the entire dataset to get all time values for a single location. `:sadface:`\n",
    "\n",
    "We're going to move forward here with the `lat` and `lon` dimensions being chunked at **354**\n",
    "observations each. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size Considerations\n",
    "\n",
    "Shape is only part of the equation.  Total chunk size matters also.  Size considerations\n",
    "come into play mostly as a consideration of how the chunks are stored on disk. The \n",
    "retrieval time is influenced by the size of each chunk. Here are some constraints: \n",
    "\n",
    "* Files Too Big -- In a zarr dataset, each chunk is stored as a separate binary file. \n",
    "  If we need data from a particular chunk, no matter how little or how much, that file gets \n",
    "  opened, decompressed, and the whole thing read into memory. A large chunk sizes means \n",
    "  that there may be a lot of data transferred in situations when only a small subset of \n",
    "  that chunk's data is actually needed.  It also means there might not be enough chunks \n",
    "  to allow the dask workers to stay busy loading data in parallel. \n",
    "* Files Too Small -- If the chunk size is too small, the time it takes to read and \n",
    "  decompress the data for each chunk can become comparable to the latency of S3 (typically \n",
    "  10-100ms). We want the reads to take at least a second or so, so that the latency is\n",
    "  not a significant part of the overall timing.\n",
    "  \n",
    "As a general rule, aim for chunk sizes between 10 and 200MB, depending on shape\n",
    "and expected read pattern (see below). Expect 100ms latency for each separate\n",
    "chunk that a process needs to open.\n",
    "\n",
    "### Total Chunk Size\n",
    "Now that we have a rough idea of the chunk dimensions, let's compute its size in bytes.  \n",
    "This will tell us if we've hit our target of between 10 and 200MB per chunk.  More \n",
    "importantly, it will tell us if we will overwhelm the OpenDAP server -- the server \n",
    "can only give us 500MB at a time. Chunks should really be smaller than this (which \n",
    "we want anyway, but this 500MB limit is a hard cut-off). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#       lat   lon  time  float32\n",
    "bytes = 354 * 354 * 72 * 4\n",
    "kbytes = bytes / (2**10)\n",
    "mbytes = kbytes / (2**10)\n",
    "print(f\"TMN chunk size: {bytes=} ({kbytes=:.2f}) ({mbytes=:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're looking really good for size.  Maybe even a bit low.\n",
    "But we're in the (admitedly broad) range of 10-200 megabytes of\n",
    "uncompressed data (i.e. in-memory) per chunk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the chunk plan\n",
    "Now that we know how we want to chunk the data, we need to give that \n",
    "information to the API which will read the data from its OpenDAP endpoint: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_in = xr.open_dataset(\n",
    "    OPENDAP_url, \n",
    "    decode_times=False, \n",
    "    chunks={ #this directs the open_dataset method to structure its reads in a particular way.\n",
    "        'time': 72, \n",
    "        'lon': 354, \n",
    "        'lat': 354\n",
    "    }\n",
    ")\n",
    "ds_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking more closely at the `tmn` variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_in.tmn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** that the display looks differently than it does in the {doc}`ExamineSourceData` notebook. \n",
    "In that original data examination (when we did not express a chunking preference), the data was \n",
    "described as `1319227560 values with dtype=float32`.  In the above data description, you can see\n",
    "that those observations are structured in the chunks that we've asked for.  Notice that the\n",
    "`xarray` description of chunk size matches our rough calculation (34.4 MB/chunk). \n",
    "\n",
    "Also, take note that the \"Attributes\" is still claming that `_ChunkSizes` is `[1 23 44]`. This\n",
    "is clearly a lie (it was never really true, actually).  This is a particular oddity with OpenDAP\n",
    "(or perhaps with this server), and won't be a consideration if you are working with data from\n",
    "other sources. \n",
    "\n",
    "We've specifically asked the dask interface to request this data according to the chunk pattern \n",
    "specified -- and this is revealed in the graphical display.  Dask will make specific OPeNDAP \n",
    "requests *per chunk* using appropriate query parameters to the server. \n",
    "\n",
    "Because this chunk pattern can be provided by the server, and it is a reasonable pattern for \n",
    "object storage in S3, we do **not** need to add the complexity of `rechunker`. We can just \n",
    "have the zarr driver write it out according to the same plan.  Even so, it is useful to \n",
    "lay out exactly what the chunking plan might be if we were using `rechunker`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_plan = {\n",
    "    'ppt':{'time': 72, 'lon': 354, 'lat': 354},    \n",
    "    'tmx':{'time': 72, 'lon': 354, 'lat': 354},    \n",
    "    'tnm':{'time': 72, 'lon': 354, 'lat': 354},\n",
    "    'time_bnds': {'time': 1, 'tbnd': 2},\n",
    "    'lat': (621,),\n",
    "    'lon': (1405,),\n",
    "    'time': (1512,),\n",
    "    'tbnd': (2,)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the coordinate variables themselves (`lat`, `lon`, etc) are stored as single-chunk stripes of data. \n",
    "Recall that these are used to translate a latitude (or longitude) value into the actual corresponding array \n",
    "address.  These coordinate arrays will always be needed in their entirity, so we chunk them such that \n",
    "they read with one chunk each. This does not affect how the data representing the data variables is chunked.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
