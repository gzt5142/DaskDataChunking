{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Source Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**OBJECTIVE**:  \n",
    "The objective of this chapter is to demonstrate how to read an existing dataset available as an OpenDAP endpoint, and translate it into a cloud-optimized zarr on S3. \n",
    "\n",
    "This notebook will take a guided tour of the input data, and show how to pick out key metadata about the structure of the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import xarray as xr\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%run ../utils.ipynb\n",
    "_versions(['xarray'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: \n",
    "OPENDAP_url = 'https://cida.usgs.gov/thredds/dodsC/prism_v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `xarray` loader is \"lazy\" -- it will read just enough of the data to make decisions about its shape, structure, etc. It will pretend like the whole dataset is in memory (and we can treat it that way), but it will only load data as required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lazy-load\n",
    "ds_in = xr.open_dataset(OPENDAP_url, decode_times=False)\n",
    "# and show it:\n",
    "ds_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"rich\" HTML output to show the `xarray.Dataset` includes a lot of information, some of which is hidden behind toggles.  Click on the icons to the right to expand and see all the metadata available for the dataset. \n",
    "\n",
    "Notable observations: \n",
    "* **Dimensions** -- This dataset is 3D, with data being indexed by `lon`, `lat`, and `time` (setting  \n",
    "  side `time_bnds` for the moment; it is a special case). Looking at the \"Dimensions\" line, you \n",
    "  can see that each of these dimensions is quantified -- how many unique values are available in \n",
    "  each dimension: \n",
    "    * **lon** = 1405\n",
    "    * **lat** = 621\n",
    "    * **time** = 1512\n",
    "* **Coordinates** -- These are the convenient handles by which dimensions can be referenced. In this \n",
    "  dataset, a coordinate can be used to pick out a particular cell of the array.  Asking for \n",
    "  cells where `lat=49.9` is possible because these coordinates map the meaningful values of latitude\n",
    "  to the behind-the-scenes cell index needed to fetch the value. \n",
    "* **Data Variables** -- The variables are `tmx`, `ppt`, and `tmn`, which are associated \n",
    "  with three indices by which data values are located in space and time (the _Dimensions_). \n",
    "* **Indexes** -- this is an internal data structure to help `xarray` quickly find items in the array.\n",
    "* **Attributes** -- Arbitrary metadata associated with the dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of the data variables to learn more about how it is presented by the OPeNDAP endpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable = \"Data Array\"\n",
    "\n",
    "Each data variable is its own N-dimensional array (in this case, 3-dimensional, indexed by lat, lon, and time).  We can look at the individual variables by examining its array separately from the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_in.tmn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note from the top line that this variable is indexed as a tuple in `(time, lat, lon)`. So, behind the scenes, there is an array whose first index is a value between 0 and 1511.  How do we know the time value of index 0? (or any index, really) The \"Coordinates\" are the lookup table to say what \"real\" time value is associated with each index address. \n",
    "\n",
    "You'll notice that the data description in this case is merely \"1319227560 values with dtype=float32\"\n",
    "with no indication as to how it is chunked. Assuming our 3-D array is fully populated, this value makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time  lat  lon\n",
    "1512 * 621 * 1405"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of chunking, this is where it gets interesting.\n",
    "\n",
    "Notice that in the data attributes, that `_ChunkSizes` gives the chunk \n",
    "sizes of the data, expressed as a tuple to match the dimensions. If we\n",
    "choose to believe this, it indicates that the data are broken into \n",
    "chunks, each of which is \n",
    "* 1 timestep, \n",
    "* 23 latitude steps, and \n",
    "* 44 longitude steps. \n",
    "\n",
    "In this case, we should be skeptical, because this information comes from an \"Attribute\", which \n",
    "may or may not be relevant.  Virtually anything can be set as an attribute on the dataset, and \n",
    "it does not affect the internal structure **AT ALL**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_in.tmn.attrs['spam'] = \"Delicious\"\n",
    "ds_in.tmn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of this OpenDAP data, we can choose to believe that this is how\n",
    "the server would like to give us the data as a default.  If we accept that\n",
    "default: \n",
    "\n",
    "Gven that `tmn` is stored as a `float32` (4 bytes), each chunk is of size: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     time  lat  lon  float32\n",
    "bytes = 1 * 23 * 44 * 4\n",
    "kbytes = bytes / (2**10)\n",
    "mbytes = kbytes / (2**10)\n",
    "print(f\"TMN chunk size: {bytes=} ({kbytes=:.2f})({mbytes=:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an **extremely** small chunk size, and not at all suitable for cloud storage.\n",
    "We certainly will want to change that when we write this data. \n",
    "\n",
    "The good news is that we are not stuck with it. The opendap server is offering us \n",
    "its default chunking for network API requests, but this is configurable. We can \n",
    "change it to something more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenDAP Considerations\n",
    "\n",
    "A subtle point about this particular dataset, given that we are reading it \n",
    "from an OpenDAP Server:  The server can't give us data in chunks bigger than\n",
    "500MB. \n",
    "\n",
    "When it comes time to read this data, we need to specify the pattern that we\n",
    "want the data in.  This will ensure that each individual data request (i.e. \n",
    "each 'chunk') is smaller than this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
